{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import logger\n",
    "import pandas as pd\n",
    "import transforms_distributed as tfd\n",
    "# import tc_distributed as tcd\n",
    "import tc_distributed_pro as tcdp\n",
    "from dask.distributed import LocalCluster, Client, progress\n",
    "from dask_jobqueue import HTCondorCluster\n",
    "import logging, coloredlogs\n",
    "import sys, os\n",
    "import copy\n",
    "import pathlib\n",
    "import warnings\n",
    "\n",
    "dataset = 'data/AP_Omentum_Ovary.csv'\n",
    "# dataset = 'data/R/Openml_6071.csv'\n",
    "datapath = 'result/' + dataset[5:-4]\n",
    "art_new, start_idx = 'C', 5\n",
    "if dataset[5] == 'R':\n",
    "    art_new, start_idx = 'R', 7\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "# logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger_new = logging.getLogger(__name__)\n",
    "# output_file_handler = logging.FileHandler(\"log/output_\" + dataset[start_idx:-3] + 'log')\n",
    "output_file_handler = logging.FileHandler(\"log/R/output_\" + dataset[start_idx:-3] + 'log')\n",
    "output_file_handler.setFormatter(formatter)\n",
    "logger_new.addHandler(output_file_handler)\n",
    "coloredlogs.install(level = 'DEBUG')\n",
    "pathlib.Path(datapath).mkdir(parents=True, exist_ok=True)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-semester",
   "metadata": {},
   "source": [
    "## Generate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = tcdp.load_data(dataset, logger=logger_new, art=art_new)\n",
    "group = dat.drop(dat.columns[-1], axis=1, inplace=False)\n",
    "res = tcdp.eval(dat.iloc[:, :-1], dat.iloc[:, -1], art=art_new)\n",
    "res, v = tcdp.compute(res)[0]\n",
    "logger_new.info(\"For original dataset %s, mean score is %f, standard variance is %f\" %(str(dataset[start_idx:-4]), res, v))\n",
    "for num in range(10, 101, 10):\n",
    "# num = 50\n",
    "    dat_sel = tcdp.bestFeatures(dat, num, art=art_new)\n",
    "    cur_dat = dat_sel\n",
    "    cur_dat[dat.columns[-1]] = dat.iloc[:, -1]\n",
    "    group = cur_dat.drop(cur_dat.columns[-1], axis=1, inplace=False)\n",
    "    res = tcdp.calculateFitness(dat, group, art=art_new)\n",
    "    res, v = tcdp.compute(res)[0]\n",
    "    logger_new.info(\"For dataset %s which selects %d features from original, its mean score is %f, standard variance is %f\" %(str(dataset[start_idx:-4]), num, res, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-siemens",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "repeat = 4\n",
    "una_oprs = tfd.unary_operators\n",
    "bina_oprs = tfd.binary_operators\n",
    "weights = [repeat] * (len(una_oprs) + len(bina_oprs))\n",
    "# import data\n",
    "dat = tcdp.load_data(dataset, logger=logger_new, art=art_new)\n",
    "# the number of next generation's features not over inflation*num_of_curr_gen to form an up-side-down pyramid structure\n",
    "# magic_number: inflation, cur_limit, total_limit, num_best_features\n",
    "inflation = 10\n",
    "cur_size = dat.shape[1]-1\n",
    "# total_limit can be much larger, which denpends on the device\n",
    "cur_limit = 2000\n",
    "total_limit = 1000\n",
    "cur_dat = dat\n",
    "prev_gen = None\n",
    "# only for the dataset with too many original features\n",
    "dat_sel = None\n",
    "\n",
    "if dat.shape[1] > 10000:\n",
    "    # for AP_Omentum_Ovary\n",
    "    group = dat.drop(dat.columns[-1], axis=1, inplace=False)\n",
    "    res = tcdp.eval(dat.iloc[:, :-1], dat.iloc[:, -1], art=art_new)\n",
    "    res, v = tcdp.compute(res)[0]\n",
    "    logger_new.info(\"For original dataset %s, mean score is %f, standard variance is %f\" %(str(dataset[start_idx:-4]), res, v))\n",
    "    num = 50\n",
    "    dat_sel = tcdp.bestFeatures(dat, num, art=art_new)\n",
    "    cur_dat = dat_sel\n",
    "    cur_dat[dat.columns[-1]] = dat.iloc[:, -1]\n",
    "    group = cur_dat.drop(cur_dat.columns[-1], axis=1, inplace=False)\n",
    "    res = tcdp.calculateFitness(dat, group, art=art_new)\n",
    "    res, v = tcdp.compute(res)[0]\n",
    "    logger_new.info(\"For dataset %s which selects %d features from original, its mean score is %f, standard variance is %f\" \n",
    "                    %(str(dataset[start_idx:-4]), num, res, v))\n",
    "\n",
    "# generate and store candidate features\n",
    "for i in range(repeat):\n",
    "    cur_dat, gen = tcdp.updateDat(cur_dat, prev_gen=prev_gen, oprs_weights=weights, art=art_new, logger=logger_new)\n",
    "    cur_gen, cur_size = tcdp.constrainFeaturesNum(cur_dat, min(inflation*cur_size, cur_limit), art=art_new, logger=logger_new)\n",
    "    logger_new.debug('The number of features in cur-gen drops from %d to %d' %(cur_dat.shape[1]-1, cur_size))\n",
    "\n",
    "    prev_gen = pd.concat([prev_gen, gen], axis=1)\n",
    "    prev_gen[dat.columns[-1]] = dat.iloc[:, -1]\n",
    "    prev_gen, prev_size = tcdp.constrainFeaturesNum(prev_gen, total_limit, art=art_new, logger=logger_new)\n",
    "    logger_new.debug('The number of features in prev-gen drops from %d to %d' %(prev_gen.shape[1], prev_size))\n",
    "\n",
    "    if dat_sel is not None:\n",
    "        cur_dat = tcdp.addInitalFeatures(cur_gen, prev_gen, dat_sel, logger=logger_new)\n",
    "    else:\n",
    "        cur_dat = tcdp.addInitalFeatures(cur_gen, prev_gen, dat, logger=logger_new)\n",
    "    cur_dat.to_csv(datapath + '/gen' + str(i+1) + '.csv')\n",
    "\n",
    "total_dat = pd.concat([prev_gen, cur_dat], axis=1)\n",
    "total_dat.to_csv(datapath + '/total.csv')\n",
    "# drop the columns with high correlations\n",
    "# total_dat = tcdp.dropHighCorrelation(total_dat, logger=logger_new)\n",
    "# total_dat[dat.columns[-1]] = dat.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-pennsylvania",
   "metadata": {},
   "source": [
    "## Select features from total and generate final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-ordinary",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dat = tcdp.load_data(dataset, art=art_new)\n",
    "gen1 = pd.read_csv(datapath + '/gen1.csv')\n",
    "gen1.drop(gen1.columns[0], axis=1, inplace=True)\n",
    "gen1.drop(gen1.columns[-1], axis=1, inplace=True)\n",
    "gen2 = pd.read_csv(datapath + '/gen2.csv')\n",
    "gen2.drop(gen2.columns[0], axis=1, inplace=True)\n",
    "gen2.drop(gen2.columns[-1], axis=1, inplace=True)\n",
    "gen3 = pd.read_csv(datapath + '/gen3.csv')\n",
    "gen3.drop(gen3.columns[0], axis=1, inplace=True)\n",
    "# gen3.drop(gen3.columns[-1], axis=1, inplace=True)\n",
    "# gen4 = pd.read_csv(datapath + '/gen4.csv')\n",
    "# gen4.drop(gen4.columns[0], axis=1, inplace=True)\n",
    "total_dat = pd.concat([gen1, gen2, gen3], axis=1)\n",
    "total_dat.to_csv(datapath + '/total' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = tcdp.load_data(dataset, art=art_new)\n",
    "# dat = pd.read_csv(dataset)\n",
    "print(dat.shape[1])\n",
    "res = tcdp.eval(dat.iloc[:, :-1],dat.iloc[:, -1], art=art_new)\n",
    "res, v = tcdp.compute(res)[0]\n",
    "print(\"For dataset %s, mean score is %f, standard variance is %f\" %(str(dataset[start_idx:-4]), res, v))\n",
    "\n",
    "total_dat = pd.read_csv(datapath + '/total.csv')\n",
    "total_dat.drop(total_dat.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_cands = tcdp.bestFeatures(total_dat, 30, art=art_new)\n",
    "init_fitness, cur_fitness = tcdp.scoreCompare(dat, best_features_cands, art=art_new, logger=logger_new)\n",
    "cur_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    i /= 10\n",
    "    num_best_features = round(i * total_dat.shape[1])\n",
    "    best_features_cands = tcdp.bestFeatures(total_dat, num_best_features, art=art_new)\n",
    "    init_fitness, cur_fitness = tcdp.scoreCompare(dat, best_features_cands, art=art_new, logger=logger_new)\n",
    "    logger_new.info(\"Finish selecting best %d features (coefficient: %f) in the first round coarsely\" %(num_best_features, i))\n",
    "    increase = (cur_fitness - init_fitness) / init_fitness\n",
    "    logger_new.debug(\"After the first round, the fitness is %f compared with the initial one, the fitness increased by %s\" %(cur_fitness, str(increase)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for best combination of coef1 and coef2\n",
    "max_inc, coef1, coef2 = 0, 0, 0\n",
    "dat_sel = None\n",
    "for i in range(1, 10):\n",
    "    i /= 10\n",
    "    num_best_features = round(i * total_dat.shape[1])\n",
    "    best_features_cands = tcdp.bestFeatures(total_dat, num_best_features, art=art_new)\n",
    "    last = best_features_cands.shape[1]\n",
    "    for j in range(10, 0, -1):\n",
    "        j /= 10\n",
    "        if dat_sel is not None:\n",
    "            size_limit = max(3, round(j * dat_sel.shape[1]))\n",
    "        else:\n",
    "            size_limit = max(3, round(j * dat.shape[1]))\n",
    "        if size_limit > last:\n",
    "            continue\n",
    "        best_features = pd.DataFrame(best_features_cands)\n",
    "        best_features[total_dat.columns[-1]] = total_dat.iloc[:, -1]\n",
    "        while best_features.shape[1] > size_limit:\n",
    "            best_features = tcdp.featureSelection(best_features, art=art_new)\n",
    "        last = best_features.shape[1]\n",
    "        best_features.drop(best_features.columns[-1], axis=1, inplace=True)\n",
    "        init_fitness, cur_fitness = tcdp.scoreCompare(dat, best_features, art=art_new)\n",
    "        increase = (cur_fitness - init_fitness) / init_fitness\n",
    "        logger_new.debug(\"with coef1=%f, coef2=%f, the increasing is %f\" %(i, j, increase))\n",
    "        if increase > max_inc:\n",
    "            max_inc, coef1, coef2 = increase, i, j\n",
    "logger_new.debug(\"After grid search of best coeffs, with coef1: %f coef2: %f, the increase maximized by %f\" %(coef1, coef2, max_inc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best K features from the total candidate features\n",
    "dat_sel = None\n",
    "coef1 = 0.9\n",
    "num_best_features = round(coef1 * total_dat.shape[1])\n",
    "best_features_cands = tcdp.bestFeatures(total_dat, num_best_features, art=art_new, logger=logger_new)\n",
    "init_fitness, cur_fitness = tcdp.scoreCompare(dat, best_features_cands, art=art_new, logger=logger_new)\n",
    "logger_new.info(\"Finish selecting best %d features (coefficient: %f) in the first round coarsely\" %(num_best_features, coef1))\n",
    "increase = (cur_fitness - init_fitness) / init_fitness\n",
    "logger_new.debug(\"After the first round, the fitness is %f compared with the initial one, the fitness increased by %s\" %(cur_fitness, str(increase)))\n",
    "\n",
    "# reduce the features number on the basis of a higher score compared to the initial one\n",
    "coef2 = 0.9\n",
    "if dat_sel is not None:\n",
    "    size_limit = max(3, round(coef2 * dat_sel.shape[1]))\n",
    "else:\n",
    "    size_limit = max(3, round(coef2 * dat.shape[1]))\n",
    "logger_new.info(\"The limit of the final gen's size is %d with coeffiecient %f\" %(size_limit, coef2))\n",
    "best_features = pd.DataFrame(best_features_cands)\n",
    "best_features[total_dat.columns[-1]] = total_dat.iloc[:, -1]\n",
    "print(size_limit)\n",
    "while best_features.shape[1] > size_limit:\n",
    "    best_features = tcdp.featureSelection(best_features, art=art_new, logger=logger_new)\n",
    "    print(\"best_features: %d\" %(best_features.shape[1]))\n",
    "best_features.drop(best_features.columns[-1], axis=1, inplace=True)\n",
    "logger_new.debug(\"Reduce the size of the final selected features to %d by featureSelection at last\" %(best_features.shape[1]))\n",
    "\n",
    "init_fitness, cur_fitness = tcdp.scoreCompare(dat, best_features, art=art_new, logger=logger_new)\n",
    "increase = (cur_fitness - init_fitness) / init_fitness\n",
    "logger_new.debug(\"At last, compared with the initial one, the fitness increased by %s\" %(str(increase)))\n",
    "best_features.to_csv(datapath + '/final' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only random forest in feature selection\n",
    "coef2 = 1\n",
    "size_limit = max(3, round(coef2 * dat.shape[1]))\n",
    "best_features = total_dat\n",
    "while best_features.shape[1] > size_limit:\n",
    "    best_features = tcdp.featureSelection(best_features, art=art_new, logger=logger_new)\n",
    "    print(\"best_features: %d\" %(best_features.shape[1]))\n",
    "best_features.drop(best_features.columns[-1], axis=1, inplace=True)\n",
    "logger_new.debug(\"Only after fs with random forest, reduce the size of all candidates to %d\" %(best_features.shape[1]))\n",
    "\n",
    "init_fitness, cur_fitness = tcdp.scoreCompare(dat, best_features, art=art_new, logger=logger_new)\n",
    "increase = (cur_fitness - init_fitness) / init_fitness\n",
    "logger_new.debug(\"With only one fs step, compared with the initial one, the fitness increased by %s\" %(str(increase)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only selectKBest in fs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-monroe",
   "metadata": {},
   "source": [
    "## The First Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-campaign",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'data/AP_Omentum_Ovary.csv'\n",
    "datapath = 'result/' + dataset[5:-4]\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "# logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger_new = logging.getLogger(__name__)\n",
    "\n",
    "output_file_handler = logging.FileHandler(\"log/output2_\" + dataset[5:-3] + 'log')\n",
    "output_file_handler.setFormatter(formatter)\n",
    "logger_new.addHandler(output_file_handler)\n",
    "coloredlogs.install(level = 'DEBUG')\n",
    "\n",
    "# stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "# stdout_handler.setFormatter(formatter)\n",
    "# logger_new.addHandler(stdout_handler)\n",
    "\n",
    "# cluster = LocalCluster(n_workers=4, threads_per_worker=2, memory_limit='2GB')\n",
    "# client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-being",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "una_oprs = tfd.unary_operators\n",
    "bina_oprs = tfd.binary_operators\n",
    "\n",
    "pathlib.Path(datapath).mkdir(parents=True, exist_ok=True)\n",
    "repeat = 1\n",
    "weights = [repeat] * (len(una_oprs) + len(bina_oprs))\n",
    "# load data\n",
    "dat = tcdp.load_data(dataset, logger=logger_new, art='C')\n",
    "cur_dat = dat\n",
    "prev_gen = None\n",
    "total_limit = 400\n",
    "prev_size = 0\n",
    "\n",
    "for i in range(repeat):\n",
    "    cur_dat.to_csv(datapath + '/gen' + str(i) + '.csv')\n",
    "    cur_dat = tcdp.update_dat(cur_dat, prev_gen=prev_gen, oprs_weights=weights, art='C', logger=logger_new)\n",
    "    cur_gen, prev_size = tcdp.pyramid_cur(cur_dat, prev_size, art='C', logger=logger_new)\n",
    "\n",
    "    prev_gen = pd.concat([prev_gen, cur_gen], axis=1)\n",
    "    # total_limit can be much larger, which denpends on the device\n",
    "    prev_gen[dat.columns[-1]] = dat.iloc[:, -1]\n",
    "    while prev_gen.shape[1] > total_limit:\n",
    "        logger.info('The number of prev_gen columns %d exceed prev_limit %d, columns selection first' % (prev_gen.shape[1], prev_limit))\n",
    "        prev_gen = tcdp.feature_selection(prev_gen, art=art, logger=logger)\n",
    "    prev_gen.drop(prev_gen.columns[-1], axis=1, inplace=True)\n",
    "\n",
    "    # add in intial features\n",
    "    if (i % 2):\n",
    "        cur_dat = pd.concat([cur_gen, dat], axis=1)\n",
    "    else :\n",
    "        cur_gen[dat.columns[-1]] = dat.iloc[:, -1]\n",
    "        cur_dat = cur_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-perth",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# delete the first column of the data after read_csv\n",
    "cur_dat = pd.read_csv('result/curr_php0iVrYT.csv')\n",
    "cur_dat.drop(cur_dat.columns[0], axis=1, inplace=True)\n",
    "cur_gen = pd.DataFrame(cur_dat.iloc[:, :-1], columns=cur_dat.columns[:-1])\n",
    "# print(cur_gen)\n",
    "prev_dat = pd.read_csv('result/prev_php0iVrYT.csv')\n",
    "prev_gen = prev_dat.drop(prev_dat.columns[0], axis=1, inplace=False)\n",
    "# prev_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-kruger",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    cur_dat, prev_gen = tcd.update_dat(cur_dat, prev_gen=prev_gen, logger=logger_new)\n",
    "    cur_dat.to_csv('result/curr_' + dataset[5:])\n",
    "    prev_gen.to_csv('result/prev_' + dataset[5:])\n",
    "logger_new.warning('+Successfully finish all steps')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python378jvsc74a57bd0363df266ed0c37ae0591ebb47ff23cc623879955e46c8735a7069950da10fb5c",
   "display_name": "Python 3.7.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "363df266ed0c37ae0591ebb47ff23cc623879955e46c8735a7069950da10fb5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}